{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9QNPrnRlN7x"
      },
      "source": [
        "# SimSyn\n",
        "\n",
        "Generate visually similar synthetic images using image captions as text prompts to a generative model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS3nrXnXkjLN"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECu34FIwc0wN"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9qe2C-ipBwx"
      },
      "outputs": [],
      "source": [
        "!mkdir images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vmrPTHSaFN_"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade diffusers transformers scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE1YuBiuaXrj"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRfbL9P1lpft"
      },
      "source": [
        "### Define Image-To-Text and Text-To-Image models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ-UbkPPkomm"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from transformers import AutoTokenizer, ViTFeatureExtractor, VisionEncoderDecoderModel\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set params\n",
        "max_length = 16\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "\n",
        "#### Setup image captioning model\n",
        "\n",
        "# taken from https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "captioner = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "captioner.to(device)\n",
        "\n",
        "\n",
        "#### Setup stable diffusion model\n",
        "\n",
        "# take from https://huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "generator = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)\n",
        "generator = generator.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsQyPkb2nHGL"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGtZE1Qolznb"
      },
      "outputs": [],
      "source": [
        "def read_resize_image(image_path, new_width=224, new_height=224):\n",
        "    \"\"\" \n",
        "    Load and resize an image to a desired size.\n",
        "    Arguments:\n",
        "        image_path (str): Image to load and resize\n",
        "        new_width (int): New width of the image\n",
        "        new_height (int): New height of the image\n",
        "    Returns:\n",
        "        img (np.array): Resized image\n",
        "    Examples:\n",
        "        >>> img = read_resize_image(\"images/doggo.jpeg\")\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(image_path) == str, f\"Should be a path, got: {image_path} which is {type(image_path)}\"\n",
        "    \n",
        "    img = Image.open(image_path)\n",
        "    img = ImageOps.fit(img, (new_width, new_height), Image.BICUBIC)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = np.array(img)\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # required when saving\n",
        "    # If need saving\n",
        "    # using cv2 - cv2.imwrite(\"path.png\", img)\n",
        "    # or using PIL - img.save(\"path.png\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "  \"\"\"\n",
        "  Usage: grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "  \"\"\"\n",
        "  assert len(imgs) == rows*cols\n",
        "\n",
        "  w, h = imgs[0].size\n",
        "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "  grid_w, grid_h = grid.size\n",
        "\n",
        "  for i, img in enumerate(imgs):\n",
        "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "  return grid\n",
        "\n",
        "def predict_caption(image_path):\n",
        "  \"\"\"\n",
        "  Predicts caption from an image.\n",
        "  \"\"\"\n",
        "  image = Image.open(image_path)\n",
        "  image = image.convert(mode=\"RGB\")\n",
        "  pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "  pixel_values = pixel_values.to(device)\n",
        "  output_ids = captioner.generate(pixel_values, **gen_kwargs)\n",
        "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "  preds = [pred.strip() for pred in preds][0]\n",
        "  return preds\n",
        "\n",
        "def predict_image(prompt):\n",
        "  \"\"\"\n",
        "  Generates an image from the input text.\n",
        "  \"\"\"\n",
        "  with autocast(\"cuda\"):\n",
        "    image = generator(prompt, height=512, width=512, guidance_scale=7.5)\n",
        "  # image.save(f\"name.png\")\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01UmX0v8mW4V"
      },
      "source": [
        "### Upload your image image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efdblMkbkopW"
      },
      "outputs": [],
      "source": [
        "%cd images\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7QOlUHmgO3"
      },
      "source": [
        "### Get caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPduGNFekou7"
      },
      "outputs": [],
      "source": [
        "image_path = os.path.join('/content/images', sorted(os.listdir('/content/images'))[0])\n",
        "caption = predict_caption(image_path)\n",
        "caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGH34JxQmitX"
      },
      "source": [
        "### Generate new images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3kDUyEETPZ0"
      },
      "outputs": [],
      "source": [
        "all_images = []\n",
        "# Do as many as you want until Colab breaks hahah!\n",
        "for i in range(5):\n",
        "    img = predict_image(caption).images[0]\n",
        "    img = np.array(img.convert(\"RGB\"))    \n",
        "    all_images.append(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Images"
      ],
      "metadata": {
        "id": "hALV18Qoqd7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, img in enumerate(all_images):\n",
        "  img = Image.fromarray(np.uint8(img)).convert('RGB')\n",
        "  img.save(f\"./images/{idx}.png\")"
      ],
      "metadata": {
        "id": "u1V_xQmfqf1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWhX_Q7KZ3-y"
      },
      "source": [
        "### Show images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = read_resize_image(image_path, new_width=512, new_height=512)\n",
        "images = Image.fromarray(np.concatenate([np.array(x) for x in all_images[:5]], axis=1))\n",
        "images = np.array(images.convert(\"RGB\"))\n",
        "\n",
        "white = np.zeros([512,200,3],dtype=np.uint8)\n",
        "white.fill(255)\n",
        "input_image_ = np.concatenate([input_image, white], axis=1)\n",
        "imgs = np.concatenate([input_image_, images], axis=1)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(60, 5))\n",
        "plt.axis(\"off\")\n",
        "plt.title(f'\"{caption}\"', fontsize=30)\n",
        "plt.title('Input Image', fontsize=30, loc='left')\n",
        "plt.imshow(imgs)\n",
        "plt.savefig(\"visualization.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8G5JnR-kpNfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7vrYrj-mV68"
      },
      "source": [
        "### Download your generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5AJwTpDmLlc"
      },
      "outputs": [],
      "source": [
        "!zip -r ./images.zip ./images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"./images.zip\")"
      ],
      "metadata": {
        "id": "qajhNKYEyRqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf images/*\n",
        "!rm -rf images.zip\n",
        "!rm -rf visualization.png"
      ],
      "metadata": {
        "id": "lAtSnk2KyQOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can upload another image and do the same thing."
      ],
      "metadata": {
        "id": "HHtcbOs5rAzW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p5uTQ3uSyIb"
      },
      "source": [
        "### Acknowledgements \n",
        "\n",
        "* https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\n",
        "* https://huggingface.co/nlpconnect/vit-gpt2-image-captioning "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xS3nrXnXkjLN",
        "pRfbL9P1lpft",
        "MGH34JxQmitX",
        "hALV18Qoqd7i",
        "u7vrYrj-mV68"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}